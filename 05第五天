第五天
之前crawler产生的数据存储在json文件中，今天我们把数据存储到数据库里。选择任意主流数据库，oracle,sql-server,mysql,redis,sqlite,mongodb等均可。
考虑并发读写性能要求并不高，采用一般关系型数据库均可，我们以开源免费的mysql为例。
首先准备一个mysql server，添加数据库crawler，在库里简单加三个表：fetches表做所有的新闻存储表，sources表做所有数据源配置表（暂不用），users表做用户登陆账户表（暂不用）。

CREATE TABLE `fetches` (
  `id_fetches` int(11)  NOT NULL AUTO_INCREMENT,
  `url` varchar(200) DEFAULT NULL,
  `source_name` varchar(200) DEFAULT NULL,
  `source_encoding` varchar(45) DEFAULT NULL,
  `title` varchar(200) DEFAULT NULL,
  `keywords` varchar(200) DEFAULT NULL,
  `author` varchar(200) DEFAULT NULL,
  `publish_date` date DEFAULT NULL,
  `crawltime` datetime DEFAULT NULL,
  `content` longtext,
  `createtime` datetime DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (`id_fetches`),
  UNIQUE KEY `id_fetches_UNIQUE` (`id_fetches`),
  UNIQUE KEY `url_UNIQUE` (`url`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

CREATE TABLE `sources` (
  `id_sources` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(200) DEFAULT NULL,
  `domain` varchar(200) DEFAULT NULL,
  `encoding` varchar(45) DEFAULT NULL,
  `seedurls` mediumtext,
  `seedurl_format` varchar(200) DEFAULT NULL,
  `type` varchar(45) DEFAULT NULL,
  `impact` int(11) DEFAULT NULL,
  `createtime` datetime DEFAULT CURRENT_TIMESTAMP,
  `title_format` varchar(200) DEFAULT NULL,
  `keywords_format` varchar(200) DEFAULT NULL,
  `date_format` varchar(200) DEFAULT NULL,
  `author_format` varchar(200) DEFAULT NULL,
  `content_format` varchar(200) DEFAULT NULL,
  PRIMARY KEY (`id_sources`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;

CREATE TABLE `users` (
  `id_users` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(45) DEFAULT NULL,
  `pass` varchar(45) DEFAULT NULL,
  `desc` varchar(45) DEFAULT NULL,
  `previlige` varchar(45) DEFAULT NULL,
  `createtime` datetime DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (`id_users`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

添加node.js的mysql模块： npm install mysql，用以下方式连接数据库

//连接数据库
var mysql = require('mysql');
var connection = mysql.createConnection({
    host: '数据库地址如:localhost',
    user: '数据库用户名',
    password: '密码',
    database:'数据库名'
});

connection.connect();
//查询
connection.query('select * from `fetches`', function(err, rows, fields) {
    if (err) throw err;
    console.log('查询结果为: ', rows);
});
//关闭连接
connection.end();

我们修改优化一下连接方式，使用连接池方式连接，单独创建一个mysql.js文件，放在e:\node_crawl\lib下

var mysql = require("mysql");

var pool = mysql.createPool({  
    host: '*.*.*.*',
    user: '*',
    password: '*',
    database: 'crawler'
});  
  
var query=function(sql,sqlparam,callback){  
    pool.getConnection(function(err,conn){  
        if(err){  
            callback(err,null,null);  
        }else{  
            conn.query(sql,sqlparam,function(qerr,vals,fields){  
                //释放连接  
                conn.release();  
                //事件驱动回调  
                callback(qerr,vals,fields);  
            });  
        }  
    });  
};

var query_noparam = function (sql, callback) {
    pool.getConnection(function (err, conn) {
        if (err) {
            callback(err, null, null);
        } else {
            conn.query(sql, function (qerr, vals, fields) {
                //释放连接  
                conn.release();
                //事件驱动回调  
                callback(qerr, vals, fields);
            });
        }
    });
};
 
exports.query = query;
exports.query_noparam = query_noparam;


我们以华夏化工网为例，http://www.hxchem.net/，编码可以看到是gb2312,找到新闻种子页面http://hxchem.net/news.php?page=0&&turn_page=

在代码中我们添加了错误捕捉，加入简单的超时停止机制和抓取限速的机制，使得爬虫工作更稳定

/* 2017.3.16 Node.JS crawler by alanfoxye*/

var myRequest = require('request')
var myCheerio = require('cheerio')
var myIconv = require('iconv-lite')
require('date-utils');
var fs = require('fs');
var mysql=require('./lib/mysql.js')

//把所有的source相关字符串和jquery格式化字符串集中，准备后期用数据库进行配置，注意用\"转义字符串中的"

var source_name = "华夏化工网";
var domain = 'http://www.hxchem.net/' 
var myEncoding = "gb2312"
var seedURLs = ['http://hxchem.net/news.php?page=0&&turn_page='];//, 'http://hxchem.net/news.php?page=1&&turn_page=', 'http://hxchem.net/news.php?page=2&&turn_page='];

var seedURL_format = "$('.mainbody>ul>li>a')";
var keywords_format = "";  //没有keywords就用source_name代替
var title_format = "$('.hq_contet>dt').text()";
var date_format = "$('.hq_date').text()";
var author_format = "";  //没有author就用source_name代替
var content_format = "$('.hq_contet>dd').eq(1).text()";

var myURL_reg = 'news/detail?id=';

//防止网站屏蔽我们的爬虫
var headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.65 Safari/537.36'
}

//request模块异步fetch url
function request(url, callback) {
    var options = {
        url: url,
        encoding: null,
        //proxy: 'http://x.x.x.x:8080',
        headers: headers,
        timeout: 180000  //增加3分钟超时停止
    }
    myRequest(options, callback)
}

//用空循环简单模拟sleep，会导致CPU上升
function sleep(sleepTime) {
    for (var start = +new Date; +new Date - start <= sleepTime;) { }
}

//遍历所有seedURLs
seedURLs.forEach(function (myseedURL, index) {
    //爬取seedURL    
    request(myseedURL, function (err, res, body) {

        try {
            //用iconv转换编码
            var html = myIconv.decode(body, myEncoding);
            //console.log(html);

            //准备用cheerio解析html
            var $ = myCheerio.load(html, { decodeEntities: true });
        }
        catch (e) {
            console.log(e);
        }
        //具体新闻url的列表
        var news_urls = [];

        //从seedURL页面获取所有新闻的url列表所处的html块

        var seedurl_news;

        try {
            seedurl_news = eval(seedURL_format);
        }
        catch (e) { console.log(e) }

        seedurl_news.each(function (i, e) {

            var myURL = "";
            try {
                //得到具体新闻url
                myURL = domain + $(e).attr("href");
            }
            catch (e) { console.log(e) }

            console.log(myURL);
            //console.log(e);

            //获取具体新闻页，注意node.js的异步模式，不保证seedurl或新闻url的执行次序！
            if (myURL != "") request(myURL, function (err, res, body) {
                //用iconv转换编码
                try {
                    var html_news = myIconv.decode(body, myEncoding);
                    //console.log(html_news);

                    //准备用cheerio解析html_news
                    var $ = myCheerio.load(html_news, { decodeEntities: true });
                }
                catch (e) {
                    console.log(e);
                }

                //动态执行format字符串，构建json对象准备写入文件或数据库
                var fetch = {};
                fetch.encoding = myEncoding;  //编码

                try {
                    if (keywords_format == "") fetch.keywords = source_name; // eval(keywords_format);  //没有关键词就用sourcename
                    else fetch.keywords = eval(keywords_format);
                }
                catch (e) { console.log(e) }

                try {
                    if (title_format == "") fetch.title = ""
                    else fetch.title = eval(title_format);  //标题
                }
                catch (e) { console.log(e) }

                try {
                    if (date_format == "") fetch.date = null
                    else fetch.date = eval(date_format);    //刊登日期            
                }
                catch (e) { console.log(e) }

                try {
                    if (author_format == "") fetch.author = source_name; //eval(author_format);  //作者
                    else fetch.author = eval(author_format);
                }
                catch (e) { console.log(e) }

                try {
                    if (content_format == "") fetch.content = "";
                    else fetch.content = eval(content_format); //.replace("\r\n" + fetch.author, "");  //内容,是否要去掉作者信息自行决定
                    //fetch.author = fetch.author.replace("作者：", "");   //作者格式修改，去掉前面的"作者："
                }
                catch (e) { console.log(e) }

                fetch.crawltime = (new Date()).toFormat("YYYY-MM-DD HH24:MI:SS");  //爬取时间
                fetch.url = myURL;

                console.log(fetch);
                ////构建存储json的文件名
                //var filename = source_name + "_" + index + "_" + i + "_" + (new Date()).toFormat("YYYY-MM-DD") + ".json";
                ////存储json
                //fs.writeFileSync(filename, JSON.stringify(fetch));

                //sql insert字符串和参数，尽量使用参数方式不用字符串拼接方式
                var fetchAddSql = 'INSERT INTO fetches(url,source_name,source_encoding,title,keywords,author,publish_date,crawltime,content) VALUES(?,?,?,?,?,?,?,?,?)';
                var fetchAddSql_Params = [fetch.url, source_name, myEncoding, fetch.title, fetch.keywords, fetch.author, fetch.date, fetch.crawltime, fetch.content];

                //执行sql，数据库中fetch表里的url属性是unique的，不把重复的url内容写入数据库
                mysql.query(fetchAddSql, fetchAddSql_Params, function (qerr, vals, fields) {
                    console.log('INSERT ID:', vals);
                });

                //sleep(5000); //每爬完1个页面休息5秒钟再爬，防止被网站黑名单，有问题！
            })
        });
    })
})

