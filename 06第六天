第六天
用初学的node.js写了五天，第六天来看看代码的问题：

node.js天生异步的机制让很多操作很高效，事件和回调函数使得很多操作不需要等待完成即可进入下一步，在crawler中我们实际上使用了三层异步的函数：
1、request种子页面，这是最外层的异步函数，程序不会等待第一个种子页面下载解析完成就会去下载解析第二个种子页面
2、request新闻页面，这是中层的异步函数，最外层的第一个种子页面下载解析完成后程序就开始下载解析第一个种子页面上的第一个新闻，
同样第二个新闻也立即开始下载解析，不会去等待第一个新闻页面下载解析完成。
3、把新闻页面的各项元素写入数据库，这是第三层的异步函数，只要有一个新闻页面下载解析完成就会去写数据库，因此不保证写入数据库的新闻次序。

通常来说这是很高效的做法，但是这导致代码可读性很差（嵌套地狱），而且这样我们最后一句代码就很尴尬了。 //sleep(5000); //每爬完1个页面休息5秒钟再爬，
防止被网站黑名单，由于爬虫程序的“礼貌”原则，是不能对一个网站同步并行爬取很多页面的，这样会被网站黑名单，提高爬虫的并行效率主要是要爬虫同时去爬取多个
网站的单个页面，对于单个网站，要减少爬取的频率。但是我们看到现在的爬虫实际上会同时爬取多个种子页面上的所有新闻链接，这对于网站来说是一个很不礼貌的做法，
甚至会被当作DOS攻击。回到这句代码，sleep(5000)放在这里会block掉mysql的response，会造成mysql写入错误，写在第三层里可以暂停数据库的写入，但却不会暂
停页面的爬取，因此完全无效。

所以我们修改了代码，用在第二层上加setTimeout(func{},3000*i)的方式，读取每个页面暂停3秒。（考虑一下为什么*i）

对于一个crawler来说需要定时去爬取页面，因此我们引入node-schedule模块, npm install node-schedule, 以天天化工网为例，每天10点和22点自动爬取页面，
代码如crawler6.

为保持node.js程序稳定运行，我们采用forever模块，全局安装，npm install -g forever, 然后执行forever start crawler6.js开始保持程序运行。
用forever stop crawler6.js 停止，forever list查看，在用户文件夹的.forever文件夹中可以查看日志。
